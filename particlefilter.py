import numpy as np
import random
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.special import gamma as gamma_func
from scipy.stats import invgamma
from scipy.special import kv
from scipy.linalg import expm
import time as t
import copy
from numpy.matlib import repmat
from matplotlib.colors import LogNorm
import pandas as pd
from tqdm import tqdm

from state_space_process import StateSpaceSimulator2
from processes import *
from kalmanfilter import *



class ParticleFilter:
    def __init__(self, kv, k_mu_BM, theta, initial_state, t0, T, num_obs):
        # self.StateSimObject = StateSimObject  # Pass the StateSimObject JUST to have access to its methods, such as, generating a gamma process over some interval, and method to calc jumps mean and cov


        ss = StateSpaceSimulator2(t0, T, num_obs, MatA=True)
        ss.define_A(flatterned_A=[0, 1, 0, theta])
        ss.define_h(flatterned_h=[0,1])
        self.StateSimObject = ss

        self.rng = ss.rng

        # self.obs_times = StateSimObject.random_observation_times
        self.obs_times = np.cumsum(self.rng.exponential(scale=1 / 10, size=num_obs+1))

        self.X = np.array([[initial_state[0]], [initial_state[1]], [initial_state[2]]])
        self.langevin_A = np.array([ [0, 1], [0, theta] ])
        self.kv = kv
        self.k_mu_BM = k_mu_BM
        self.var = 1  # TODO: inheritance!
        self.sigma_mu_sq = self.var  # self.k_mu_BM * self.var

        self.dynamic_skew_var = self.k_mu_BM * self.var

        self.caligA = np.zeros((3, 3))
        self.caligB = np.array([[1, 0], [0, 1], [0, 0]])
        self.caligH = np.array([[1, 0, 0]])

        # Parameters for Dynamic Skew one-time-step-update
        self.D = None
        self.Lambda = None
        self.P = None
        self.M = None
        self.e_state_cov = None  # e_state ~ N(0, S) where S = sigma_w^2 * Ce_state , Ce_state = C1 + C2 etc...

        # initialise kalman mean and covariance vec and matrix
        self.kalman_mean = np.zeros((3, 1))  # TODO: Change to more informative prior? run a few processes first (good to include in report...)!!!
        self.kalman_cov = 0.8 * np.eye(3)
        self.kalman_gain = np.zeros((3, 1))

        # storing the noisy obs and true sate underlying evolutions of a process run (generated by calling the Kalman filter)
        self.noisy_obs = None
        self.x_evolution = None

        self.pf = self.PFSamplingUtils(number_particles=10, data_set=None, true_state_paths=None,
                                       obs_times=self.obs_times,
                                       calig_params=[self.caligH, self.caligA, self.caligB, self.kv],
                                       inverse_gauss_params=[1e-05, 1e-05],
                                       rng=np.random.default_rng(150))

    class PFSamplingUtils:

        # ~ 7 mins for 100 particles

        def __init__(self, number_particles, data_set, true_state_paths, obs_times, calig_params,
                     inverse_gauss_params, rng):
            self.Np = number_particles
            self.data_set_y_noisy_obs = data_set
            self.true_x_evo = true_state_paths
            self.obs_times = obs_times
            self.rng = rng

            self.state_evo_dict = None

            self.caligH, self.caligA, self.caligB, self.kv = calig_params
            self.rho, self.eta = inverse_gauss_params

            self.epsilon = 0.5

            self.particle_set = None

        def update_PF_attributes(self, data_set,
                                 true_state_paths):  # This is called once we run the Kalman Filter once as we collect the noisy obs of that path realisation (ONCE WE HAVE REAL DATA-SET this wont be caled as we feed in data_set to the argument of the init function !)
            self.data_set_y_noisy_obs = data_set
            self.true_x_evo = true_state_paths

        def log_sumexp_util(self, lw, h, x, axis=0, retlog=False):

            "Implementing Eqn 4.1.14"

            c = np.max(lw)
            broad_l = np.broadcast_to((lw - c).flatten(), x.T.shape).T

            if retlog:
                return c + np.log(np.sum(np.exp(broad_l) * h(x), axis=axis))
            else:
                return np.exp(c) * np.sum(np.exp(broad_l) * h(x), axis=axis)

        def initialise_particle_state_and_ktx(self, kv, k_cov_init, i):

            # a_prior = np.array([[131], [0], [0]])
            a_prior = np.array([[0], [0], [0]])
            C_prior = k_cov_init * np.eye(3)
            C_prior_chol = np.linalg.cholesky(C_prior)

            sampled_state = a_prior + (C_prior_chol @ self.rng.standard_normal(3).reshape(-1, 1))
            self.particle_set[i]['X'].append(sampled_state)

            # initial_covariance = np.zeros((3,3))
            initial_covariance = C_prior  # test - marginalising
            # self.particle_set[i]['kt_x'].append(self.caligH @ initial_covariance @ self.caligH.T + kv)
            # self.particle_set[i]['kt_x'].append(kv)

        def normalise_weights(self):

            lweights = np.array([self.particle_set[particle_num]['log-weight'] for particle_num in
                                 list(self.particle_set.keys())]).flatten().reshape(-1,1)

            sumweights = self.log_sumexp_util(lweights, lambda x: 1, np.ones(lweights.shape[0]), retlog=True)

            for particle_num in list(self.particle_set.keys()):
                self.particle_set[particle_num]['log-weight'] = self.particle_set[particle_num]['log-weight'] - sumweights

        def calculate_ESS(self, method):
            # lweights = an array of all the particles log weights
            lweights = np.array([self.particle_set[particle_num]['log-weight'] for particle_num in
                                 list(self.particle_set.keys())]).flatten()

            # these 2 method all are in the log weight domain
            if method == 'd_inf':
                log_ESS = -np.max(lweights)
            elif method == 'p2':
                log_ESS = -1 * self.log_sumexp_util(lw=2 * lweights, h=lambda x: 1., x=np.ones(lweights.shape[0]),
                                                    retlog=True)
            else:
                raise ValueError(
                    'Invalid ESS Method Provided: Please provide \'d_inf\' or \'p2\' in calculate_ESS(.)')

            return log_ESS

        def update_particle_K_attributes(self, num, attr_type, gain, mean, cov):
            if attr_type == 'predictive':
                key = 'K_predictive_density'

            else:
                key = 'K_correction_density'

            self.particle_set[num][key]['mean'] = mean
            self.particle_set[num][key]['cov'] = cov
            self.particle_set[num]['K_gain'] = gain

        def predict_y(self):
            y_pred = self.caligH @ self.particle[num]['K_predictive_density']['mean']

            return y_pred

        def compute_state_posterior(self):

            lweights = np.array([self.particle_set[particle_num]['log-weight'] for particle_num in
                                 list(self.particle_set.keys())]).flatten()

            means = np.array([self.particle_set[particle_num]['K_correction_density']['mean']
                              for particle_num in list(self.particle_set.keys())])

            msum = self.log_sumexp_util(lweights.reshape(-1,1), lambda x: x, means, axis=0, retlog=False)

            cov_term = np.array([self.particle_set[particle_num]['K_correction_density']['cov']
                                 + (self.particle_set[particle_num]['K_correction_density']['mean'] @
                                    self.particle_set[particle_num]['K_correction_density']['mean'].T)
                                 for particle_num in list(self.particle_set.keys())])

            csum = self.log_sumexp_util(lweights.reshape(-1,1), lambda x: x, cov_term, axis=0, retlog=False)

            a_mix_t = msum
            c_mix_t = csum - msum @ msum.T

            return a_mix_t, c_mix_t

        def estimate_marginalised_var(self, particle_num):

            count = self.particle_set[particle_num]['count']
            E = self.particle_set[particle_num]['E'][-1]

            rhod = 1e-05 + count / 2
            etad = 1e-05 + E / 2
            IGMean = ((1e-05 + E / 2) / (1e-05 + count / 2 - 1))
            IGMode = ((1e-05 + E / 2) / (1e-05 + count / 2 + 1))

            self.particle_set[particle_num]['mode-var-estimate'].append(IGMode)
            self.particle_set[particle_num]['mean-var-estimate'].append(IGMean)

            return IGMode


    def pf_predict_y(self, particle):
        y_pred = self.caligH @ particle['K_predictive_density']['mean']
        return y_pred

    def pf_calc_ktx(self, particle):  # my kt_x is JJs Cyt
        # print('Calculating kt_x....')
        ktx = self.caligH @ particle['K_predictive_density']['cov'] @ self.caligH.T + self.kv
        return ktx

    def pf_calc_Et(self, particle, y):
        Et = particle['E'][-1] + ((y - particle['y_pred'][-1]) ** 2 / particle['kt_x'][-1])
        return Et

    def pf_calc_log_weight(self, particle):
        ktx = particle['kt_x'][-1]
        Et = particle['E'][-1]
        Et_prev = particle['E'][-2]

        rho = self.pf.rho
        eta = self.pf.eta

        # particle['count'] += 1

        count = particle['count']

        lw = (-0.5 * np.log(ktx) - (rho + (count / 2.))) * np.log(eta + Et / 2.) + \
             (rho + ((count - 1) / 2.)) * np.log(eta + Et_prev / 2.)
        return lw

    def pf_multinomial_resample(self):

        print('RE-SAMPLING!')

        lweights = np.array([self.pf.particle_set[particle_num]['log-weight'] for particle_num in
                             list(self.pf.particle_set.keys())]).flatten()

        weights = np.exp(lweights)

        probabilities = np.nan_to_num(weights)
        probabilities = probabilities / np.sum(probabilities)

        # numpy multivariate; arguemts = number trials (N) and pvals - probabilities of each of the p different outcomes
        # the output is a vector of the drawn samples, where the value X_i = [X_0, X_1, ..., X_p] represent the number of times the outcome was i
        selections = np.random.multinomial(self.pf.Np, probabilities)
        new_particles = {}
        # new_particles = []
        #
        # for idx in range(self.pf.Np):
        #     for _ in range(selections[idx]):
        #         new_particles.append(copy.deepcopy(self.pf.particle_set[idx+1]))
        #
        # self.pf.particle_set = {i+1: d for i, d in enumerate(new_particles)}
        #
        # for particle_num in list(self.pf.particle_set.keys()):
        #     self.pf.particle_set[particle_num]['log-weight'] = -np.log(self.pf.Np)

        k = 1
        j = 1
        # k = particle number (starts at 1), j = NEW particle number (starts at 1) & is the key of the dictionary
        for selection in selections:
            if selection != 0:
                for _ in range(selection):
                    # new_particle = {}
                    # for key, value in self.pf.particle_set[k].items():
                    #     new_particle[key] = value
                    # new_particle['log-weight'] = -np.log(self.pf.Np)
                    # new_particles[j] = new_particle

                    new_particle = copy.deepcopy(self.pf.particle_set[k])
                    new_particle['log-weight'] = -np.log(self.pf.Np)
                    new_particles[j] = new_particle
                    j += 1
            k += 1

        self.pf.particle_set = new_particles

    # --------------------------------------------------

    def hist_record_states_kalman_mean(self, kalman_state_dict, particle_num, kalman_mean):
        kalman_state_dict[particle_num].append(kalman_mean)
        return kalman_state_dict

    def compute_CIs(self, means, covs, stdevs, state):

        mid = []
        lower = []
        upper = []

        if state == 'x0':
            idx = 0
        elif state == 'x1':
            idx = 1
        elif state == 'x2':
            idx = 2
        else:
            raise ValueError('Please Enter a Valid State: x0 or x1 as argument to function call')

        for mean, cov in zip(means, covs):
            x_mean = mean[idx][0]
            x_stdev = np.sqrt(cov[idx][idx])

            mid.append(x_mean)
            lower.append(x_mean - stdevs * x_stdev)
            upper.append(x_mean + stdevs * x_stdev)
        return lower, upper, mid

    # --------------------------------------------------

    def kalman_predictive_mean(self, t_start, t_end, jumps_mean):
        # update the kalman mean:
        self.kalman_mean = self.caligA @ self.kalman_mean
        return None

    def kalman_predictive_cov(self, var, jumps_cov):
        var = 1  # marginalising
        self.kalman_cov = self.caligA @ self.kalman_cov @ self.caligA.T + var * self.caligB @ jumps_cov @ self.caligB.T
        return None

    def kalman_predict(self, t_start, t_end, var, jumps_mean, jumps_cov):

        self.kalman_predictive_mean(t_start, t_end, jumps_mean)  # updates the kalman_mean attribute

        self.kalman_predictive_cov(var, jumps_cov)  # updates the kalman_cov attribute

        return None

    def compute_kalman_gain(self, var, kv):
        var = 1  # marginalising
        scaling = self.caligH @ self.kalman_cov @ self.caligH.T + var * kv
        self.kalman_gain = (self.kalman_cov @ self.caligH.T) * (scaling ** (-1))

    def kalman_update_mean(self, y):
        self.kalman_mean = self.kalman_mean + self.kalman_gain @ (y - self.caligH @ self.kalman_mean)
        return None

    def kalman_update_cov(self):
        self.kalman_cov = self.kalman_cov - self.kalman_gain @ self.caligH @ self.kalman_cov
        return None

    def kalman_update(self, y, var, kv, t_start, t_end):

        self.compute_kalman_gain(var, kv)  # uses the kalman PREDICTED cov and mean to update the kalman gain parameter
        self.kalman_update_mean(
            y)  # uses the kalman gain and PREDICTED mean and cov to UPDATE the kalman_mean attribute
        self.kalman_update_cov()  # uses the old kalman cov and gain to UPDATE the kalman_cov attribute

        return None

    # --------------------------------------------------

    def produce_I_matrices(self, start_time, end_time, jump_time_set):

        if len(jump_time_set) == 0:
            jump_time_set = [(0, start_time), (0, end_time)]
        # self.calculate_M_and_P_matrix(jump_time_set, end_time)
        self.calculate_P_and_M_matrices_2(jump_time_set, end_time)

        self.produce_skew_matrices(start_time, end_time, jump_time_set)

    def calculate_P_and_M_matrices_2(self, jump_time_set, end_time):

        jumps, times = np.array(jump_time_set).T
        times = times + [end_time]
        h = np.array([[0], [1]])
        # sigma_w = self.var
        sigma_w = 1 # marginalised form?

        M = np.array([[], []])
        P = np.array([[], []])

        jt_set = jump_time_set + [(0, end_time)]
        for jump_time_tuple in jt_set:
            jump = jump_time_tuple[0]
            time = jump_time_tuple[1]
            expA_times_h = expm(self.langevin_A * (end_time - time)) @ h

            M_entry = expA_times_h * jump

            M = np.append(M, M_entry, axis=1)

            P_entry = expA_times_h * np.sqrt(jump * (sigma_w**2))

            P = np.append(P, P_entry, axis=1)
        self.M = M
        self.P = P

    def produce_skew_matrices(self, start_time, end_time, jump_time_set):

        jtimes = [start_time] + [pair[1] for pair in jump_time_set] + [end_time]
        tdiffs = np.diff(jtimes)

        self.calculate_D_matrix(tdiffs)
        self.calculate_Lambda_matrix(tdiffs)

    def calculate_D_matrix(self, tdiffs):
        self.D = np.tri(len(tdiffs), len(tdiffs))
        pass

    def calculate_Lambda_matrix(self, tdiffs):
        # self.Lambda = self.sigma_mu_sq * np.diag(tdiffs)

        sigma_mu_sq = self.k_mu_BM * 1 # the 1 should be self.var however we say var = 1 as we marginalised it
        self.Lambda = sigma_mu_sq * np.diag(tdiffs)

        pass

    def compute_caligA_dynamic_skew(self, end_time, start_time):
        self.caligA = np.block([[expm(self.langevin_A * (end_time - start_time)), self.M @ np.ones(((np.shape(self.M))[1], 1))],
                                [np.zeros((1, 2)), 1.]])

    def compute_noise_vector_dynamic_skew(self):
        # alpha_t = A alpha_s + B e_state

        # noise vector = e_state ~ N(0, S)
        # S = C1 + C2 : C1 = k_mu_BM * sigma_w_sq * [ [  ] [   ] [  ] ] , C2 = sigma_w_sq * [ [P I P^T 0] [0 0 0] ]
        # S = sigma_w_sq * [ C1 + C2 ]


        # 1/ compute C1
        D_lambda_D = self.D @ self.Lambda @ self.D.T
        MD_lambda_D = self.M @ D_lambda_D
        D_lambda_DM = D_lambda_D @ self.M.T

        # C1 = self.k_mu_BM * np.block([[ MD_lambda_D @ self.M.T, MD_lambda_D[:, [-1]] ],
        #                               [ D_lambda_DM[-1,:],  D_lambda_D[-1, -1] ]])

        C1 = np.block([[MD_lambda_D @ self.M.T, MD_lambda_D[:, [-1]]],
                                      [D_lambda_DM[-1, :], D_lambda_D[-1, -1]]])

        # 2/ compute C2
        C2 = np.block( [ [self.P @ np.eye(np.shape(self.P)[1]) @ self.P.T, np.zeros((2,1))], [np.zeros((1,3))]] )

        # 3/ add them and times by sigma_w_s1 !

        # var = self.var
        var = 1
        S = var * (C1 + C2)

        self.e_state_cov = S

        # e = np.random.multivariate_normal([0,0,0],S)
        # e = np.reshape(e, (3,1))

        if self.k_mu_BM == 0:
            try:
                cov_chol = np.linalg.cholesky(np.block([self.P @ np.eye(np.shape(self.P)[1]) @ self.P.T]))
                e = cov_chol @ np.column_stack([self.pf.rng.normal(size=2)]) + np.zeros(
                    (2, 1))  # used to have + mean here - but now zeros
            except np.linalg.LinAlgError:
                # truncate innovation to zero if the increment is too small for Cholesky decomposition
                # print('Chol Truncated.')
                e = np.zeros((2, 1))
            e = np.append(e, 0).reshape(3, 1)
        else:
            try:
                cov_chol = np.linalg.cholesky(S)
                e = cov_chol @ np.column_stack([self.pf.rng.normal(size=3)]) + np.zeros(
                    (3, 1))  # used to have + mean here - but now zeros
            except np.linalg.LinAlgError:
                # truncate innovation to zero if the increment is too small for Cholesky decomposition
                # print('Chol Truncated.')
                e = np.zeros((3, 1))

        return e

    def update_state_vec_3d_dynamic_skew(self, e):
        self.X = self.caligA @ self.X + e
        return self.X

    # --------------------------------------------------

    def compute_noise_vector(self, var, cov_mat):
        # random noise vector is a sample from a 2D gaussian: N(0, sigma^2 S^~) where S^~ is teh covariance of teh jumps
        # var is the variance of the process
        try:
            cov_chol = np.linalg.cholesky(var * cov_mat)
            e = cov_chol @ np.column_stack([self.pf.rng.normal(size=2)]) + np.zeros((2,1)) # used to have + mean here - but now zeros
        except np.linalg.LinAlgError:
            # truncate innovation to zero if the increment is too small for Cholesky decomposition
            # print('Chol Truncated.')
            e = np.zeros((2,1))
        return e

    def compute_caligA(self, start_time, end_time, mean_vec):
        A = self.langevin_A
        self.caligA = np.block([[expm(A*(end_time - start_time)),   mean_vec],
                                [np.zeros((1, 2)),                      1.]])

    def update_state_vec_3d(self, e, t_start, t_end, jumps_mean):
        self.compute_caligA(t_start, t_end, jumps_mean)
        # we compute caligA for each fresh set of jumps; do NOT call compute caligA in the Kalman Functions; else we re-compute for no reason
        self.X = self.caligA @ self.X + self.caligB @ e
        return self.X

    # --------------------------------------------------

    def runKalmanFilterPFAdapted_DynamicSkewClosedForm(self, y, beta, t_obs, particle_number, kalman_state_evo_dict):

        obs_times = t_obs
        particle_num = particle_number
        particle = self.pf.particle_set[particle_number]

        # this fn takes in a particle. need to re-update the existing kalman gains, cov and mean of this 'FilterProcess' class for the correct particle
        # these attributes are always set to the particles updated Cov and Mean, because these are computed at the end of the time interval (when we see the new obs) and
        # so are fed into the next tiem interval to calculate the predicted estimates
        self.kalman_gain = self.pf.particle_set[particle_number]['K_gain']
        self.kalman_cov = self.pf.particle_set[particle_number]['K_correction_density'][
            'cov']  # make these the 'corrected' ones, as upon entering each new particle, we want to access the last most recent values ie the 'corrected' densoty (in the simple kalman case, we compute the predictive params first, where these use the previously corrected params to do so
        self.kalman_mean = self.pf.particle_set[particle_number]['K_correction_density']['mean']

        self.X = self.pf.particle_set[particle_number]['X'][-1]

        kalman_state_evo_dict = self.hist_record_states_kalman_mean(kalman_state_evo_dict, particle_num,
                                                                    self.X)

        # self.X = self.pf.particle_set[particle_number]['X'][-1] # retrieve last state vector for given particle

        start_time = obs_times[0]
        end_time = obs_times[1]

        step_gamma_obj = GammaDistr(alpha=1, beta=beta)
        step_gamma_obj.set_process_conditions(t0=start_time, T=end_time, END=None, sample_size=450)
        step_gamma_sim = DistributionSimulator(step_gamma_obj)
        step_gamma_path, step_gamma_time, step_gamma_jump_time_set = step_gamma_sim.process_simulation()
        # latent_gamma_jump_time_set.append(step_gamma_jump_time_set[0])
        # latent_gamma_path.append(step_gamma_path[0])

        self.produce_I_matrices(start_time, end_time, step_gamma_jump_time_set)
        # mean_I, cov_I = self.calculate_I_mean_cov()

        self.compute_caligA_dynamic_skew(end_time, start_time)

        e = self.compute_noise_vector_dynamic_skew()

        # recent: MOVED !!!!!!!!!!!
        mean = None
        self.kalman_predict(start_time, end_time, self.var, mean, self.e_state_cov)
        self.pf.update_particle_K_attributes(particle_num, 'predictive', self.kalman_gain, self.kalman_mean,
                                             self.kalman_cov)

        new_state = self.update_state_vec_3d_dynamic_skew(e) # propogate state vector

        self.pf.particle_set[particle_num]['X'].append(new_state)

        self.pf.state_evo_dict = kalman_state_evo_dict

        # x_evolution = self.StateSimObject.record_state_evolution(x_evolution, new_state)

        # mean = None
        # self.kalman_predict(start_time, end_time, self.var, mean, self.e_state_cov)
        # TODO: MAKE SURE I ENTER RIGHT (MEAN AND) COV !!!
        ######## used to have predict-update here

        # y = self.observe_state()
        # argument y below is our 'noisy observation' (from the KF (to generate our atrificial data) OR our real data we ran before)
        self.kalman_update(y, self.var, self.kv, start_time,  end_time)  # updates the kalman C so that in the next time we come in to PREEDICT, ie, have seen a new data point arrive, we use the UPDATED estimate as out t-1 estimate etc
        self.pf.update_particle_K_attributes(particle_num, 'updated', self.kalman_gain, self.kalman_mean, self.kalman_cov)


        return None


    def runKalmanFilterStepsPFAdapted(self, y, beta, t_obs, particle_number, kalman_state_evo_dict):
        obs_times = t_obs
        particle_num = particle_number
        particle = self.pf.particle_set[particle_number]

        # this fn takes in a particle. need to re-update the existing kalman gains, cov and mean of this 'FilterProcess' class for the correct particle
        # these attributes are always set to the particles updated Cov and Mean, because these are computed at the end of the time interval (when we see the new obs) and
        # so are fed into the next tiem interval to calculate the predicted estimates
        self.kalman_gain = self.pf.particle_set[particle_number]['K_gain']
        self.kalman_cov = self.pf.particle_set[particle_number]['K_correction_density'][
            'cov']  # make these the 'corrected' ones, as upon entering each new particle, we want to access the last most recent values ie the 'corrected' densoty (in the simple kalman case, we compute the predictive params first, where these use the previously corrected params to do so
        self.kalman_mean = self.pf.particle_set[particle_number]['K_correction_density']['mean']

        kalman_state_evo_dict = self.hist_record_states_kalman_mean(kalman_state_evo_dict, particle_num,
                                                                    self.kalman_mean)

        self.X = self.pf.particle_set[particle_number]['X'][-1]

        start_time = obs_times[0]
        end_time = obs_times[1]

        step_gamma_obj = GammaDistr(alpha=1, beta=beta)
        step_gamma_obj.set_process_conditions(t0=start_time, T=end_time, END=None, sample_size=450)
        step_gamma_sim = DistributionSimulator(step_gamma_obj)
        step_gamma_path, step_gamma_time, step_gamma_jump_time_set = step_gamma_sim.process_simulation()

        mean, cov = self.StateSimObject.calculate_jumps_raw_mean_and_cov(step_gamma_jump_time_set, start_time,
                                                                         end_time)

        e = self.compute_noise_vector(self.var, cov)

        self.kalman_predict(start_time, end_time, self.var, mean, cov)

        new_state = self.update_state_vec_3d(e, start_time, end_time,
                                             mean)  # this also re-calculates self.caligA for a new particle's set of jumps (uses the MEAN as input)

        self.pf.particle_set[particle_num]['X'].append(new_state)  # TODO: Append new state

        self.pf.state_evo_dict = kalman_state_evo_dict

        # x_evolution = self.StateSimObject.record_state_evolution(x_evolution, new_state)

        # self.kalman_predict(start_time, end_time, self.var, mean, cov) --- MOVED UP TO ABOVE TO TEST CHANGES
        self.pf.update_particle_K_attributes(particle_num, 'predictive', self.kalman_gain, self.kalman_mean,
                                             self.kalman_cov)

        # y = self.observe_state()
        # argument y below is our 'noisy observation' (from the KF we ran before)
        self.kalman_update(y, self.var, self.kv, start_time, end_time)  # updates the kalman C so that in the next time we come in to PREEDICT, ie, have seen a new data point arrive, we use the UPDATED estimate as out t-1 estimate etc
        self.pf.update_particle_K_attributes(particle_num, 'updated', self.kalman_gain, self.kalman_mean,
                                             self.kalman_cov)

        return None





    def run_particle_filter(self, Np, beta, k_cov_init, X0_INIT, state_obs_and_times,
                            show_particle_paths, dynamic_skew, known_real_states=False):

        if dynamic_skew['Dynamic']:
            print('Check')
            self.caligB = np.eye(3)

        print()
        print('BETA SET TO 1 in DYNAMIC SKEW CLOSED FORM KF FUNCTION, and set intial cov very low in initialse_particle_state_and_ktx')
        print()

        pf = self.pf
        pf.Np = Np
        num_particles = pf.Np
        # k_cov_init = 0.5 This was just to run a test
        self.pf.particle_set = {particle_no: {'X':[], 'log-weight': -np.log(num_particles), 'E': [0], 'kt_x':[0], 'count':0, 'mode-var-estimate':[], 'mean-var-estimate':[],
                                         'y_pred': [], 'K_gain': np.zeros((3,1)), 'K_predictive_density': {'mean': np.array([[X0_INIT], [0], [0]]), 'cov': k_cov_init*np.eye(3)},
                                         'K_correction_density': {'mean': np.array([[X0_INIT], [0], [0]]), 'cov': k_cov_init*np.eye(3)}}
                            for particle_no in list(np.arange(1,num_particles+1,1))}

        particle_evo_dict = {}

        pf.true_x_evo = known_real_states

        for i in range(1, num_particles+1):
            pf.initialise_particle_state_and_ktx(self.kv, k_cov_init, i) #TODO: initialise particle from prior - Initialise 'k_x' as H@C_1@H + kv, initialise y_pred as H@A1|1. Initialise the means and covs as: [0, 0, 0], and np.zeros(3,3)
            particle_evo_dict[i] = []

        if state_obs_and_times == 'pre-gen':
            times = pf.obs_times # THESE ARE THE FULL COLLECTION OF OBSERVATION TIMES
            observation_data = pf.data_set_y_noisy_obs
        elif state_obs_and_times != 'pre-gen':
            # UNZIP argument to extract the state obs and times
            times = state_obs_and_times[0]
            observation_data = state_obs_and_times[1]

        print('CHECK INITIALISATION OF KALMAN PRED AND UPDT MEAN AND COV IN THE PARTICLE SET DICTIONARY! CHECK ORDER OF HOW THINGS ARE UPDATED')

        state_mean = [np.array([[X0_INIT], [0], [0]])]
        state_cov = [k_cov_init*np.eye(3)]
        var_estimate_list = []
        ESS_list = []

        var_est_pi = {}
        for i in range(self.pf.Np):
            var_est_pi[i] = []

        print('CHECK: Do we start count at 0 or 1? ie is the first sampled state t=0 or t=1, and therefore, should count == the length of the final state vector?')
        with tqdm(total=len(times)*self.pf.Np) as pbar:

            for t in range(1, len(times)):
                start_time = times[t - 1]
                end_time = times[t]
                # print()
                # print('-------------')
                # print('time = {}'.format(end_time))
                # print('-------------')

                noisy_observation = observation_data[
                    t]  # our observation is at the time interval end as we are trying to predict for the end of the time interval, before seeing the 'true' state evolution

                for pn in range(1, num_particles + 1):
                    self.pf.particle_set[pn]['count'] += 1

                for i in range(1, num_particles + 1):  # i = particle number (i: 1 -> Np)

                    # if i == num_particles / 5 or i == 2 / 5 * num_particles or i == 3 / 5 * num_particles or i == 4 / 5 * num_particles or i == num_particles:
                    #     print('PARTICLE NUM: {} / {}'.format(i, num_particles))

                    # simulate jumps in interval t-1 -> t (a particle over this time period)
                    # calculate PREDICTED kalman a and C
                    # observe new OBSERVATION (noisy) data point y_i
                    # calculate UPDATED kalman paramaters a and C

                    if not dynamic_skew['Dynamic']:
                        sigma_squared_mu = 'NotDynamic'
                        self.runKalmanFilterStepsPFAdapted(y=noisy_observation, beta=beta, t_obs=[start_time, end_time],
                                                           particle_number=i, kalman_state_evo_dict=particle_evo_dict)
                    else:
                        # This is the value which WE control, when WE run the PF
                        sigma_squared_mu = self.dynamic_skew_var
                        # self.runKalmanFilterStepsPFAdapted_JumpByJumpSkew(y=noisy_observation, t_obs=[start_time, end_time], particle_number=i, kalman_state_evo_dict=particle_evo_dict, skew_var_controlled=sigma_squared_mu)
                        # self.runKalmanFilterStepsPFAdapted_TwoStepProcedure(y=noisy_observation, t_obs=[start_time, end_time], particle_number=i, kalman_state_evo_dict=particle_evo_dict, skew_var_controlled=sigma_squared_mu)

                        self.runKalmanFilterPFAdapted_DynamicSkewClosedForm(y=noisy_observation, beta=beta,
                                                                            t_obs=[start_time, end_time], particle_number=i,
                                                                            kalman_state_evo_dict=particle_evo_dict)

                    y_hat = self.pf_predict_y(particle=self.pf.particle_set[i])
                    self.pf.particle_set[i]['y_pred'].append(y_hat)

                    ktx = self.pf_calc_ktx(particle=self.pf.particle_set[i])
                    self.pf.particle_set[i]['kt_x'].append(ktx)

                    # CALCULATE UN-NORM SMC WEIGHT FOR THE JUMPS 1 -> t for this particle i using marginal sigma. sq case

                    Et = self.pf_calc_Et(particle=self.pf.particle_set[i], y=noisy_observation)
                    self.pf.particle_set[i]['E'].append(Et)

                    # HERE HERE WE now impliment that fat marginalised sigma squared equation ... as we have calculated the Et and the ktx necessary for such expression

                    # pf.particle_set[i]['count'] += 1
                    # print('count = {}'.format(self.pf.particle_set[i]['count']))
                    log_weight = self.pf_calc_log_weight(particle=self.pf.particle_set[i])
                    self.pf.particle_set[i]['log-weight'] += log_weight  # update log weight; in w_new = w_old * p -> log domain -> log_w = log_w_old + log p

                    var_est_pi[i-1].append(self.pf.estimate_marginalised_var(particle_num=i))


                    pbar.update(1)

                    # print('len X for particle {} = {}'.format(i, len(self.pf.particle_set[i]['X'])))
                # calculate Lt here (optional?)

                self.pf.normalise_weights()


                mixed_mean, mixed_cov = self.pf.compute_state_posterior()

                log_ESS = self.pf.calculate_ESS(method='d_inf')
                ESS_list.append(log_ESS)
                # print('log_ESS = {}. epsilon * N = {}'.format(log_ESS, pf.epsilon * num_particles))
                if log_ESS < np.log(pf.epsilon * num_particles):  # CHECK RESAMPLING LOGIC !!! NOT SURE IT IS DOING CORECT THING (NOTE - the < ought to be changed to > ! ) I THINK THAT WE DONT GET THE SAME NUMBER OF PARTICLES AGAIN... CHECK TOMO MORN!
                    self.pf_multinomial_resample()

                # now calculate the predictive density parameters and store a series of a_mix and C_mix so that I can plot the 'bands' !
                # Calcualte density pi(alpha_t | y_1:t) using weights as Mixture of gaussian weights. density is parameterised by some mean and covariance;

                # mixed_mean, mixed_cov = pf.compute_state_posterior()


                state_mean.append(mixed_mean)
                state_cov.append(mixed_cov)

                lweights = np.array([self.pf.particle_set[particle_num]['log-weight'] for particle_num in
                                     list(self.pf.particle_set.keys())]).flatten()
                Es = np.array([self.pf.particle_set[particle_num]['E'][-1]
                               for particle_num in list(self.pf.particle_set.keys())])
                E = self.pf.log_sumexp_util(lweights, lambda x: x, Es, retlog=False)
                count = int(self.pf.particle_set[1]['count'])
                var_estimate = (self.pf.eta + E / 2.) / (self.pf.rho + count / 2. + 1.)

                var_estimate_list.append(var_estimate[0][0])

        self.show_all_var_ests(var_est_pi)

        # gather a vector of all particles end weights.
        lweights = np.array([self.pf.particle_set[particle_num]['log-weight'] for particle_num in
                             list(self.pf.particle_set.keys())]).flatten()
        # weights =
        # gather a vector of the Inverse Gammas mean, for each particle IG(alpha, beta); IG mean = beta / alpha -1
        # IG mean = (eta + Et/2) / (rho+t/2 - 1)
        IGmeans = np.array([(self.pf.eta + (self.pf.particle_set[particle_num]['E'][-1]).flatten() / 2) / (
                    self.pf.rho + self.pf.particle_set[particle_num]['count'] / 2 - 1)
                            for particle_num in list(self.pf.particle_set.keys())])
        # use the log sum exp function, but have h(x) as the function which calculates the mean of the IG distr?
        pred_mean_var = self.pf.log_sumexp_util(lweights, lambda x: x, IGmeans, retlog=False)
        print('pred mean var. = {}'.format(pred_mean_var))

        print('VAR (MODE) ESTIMATE = {}'.format(var_estimate))



        # ESTIMTATE VAR
        particle_mean_vars = np.array([self.pf.particle_set[particle_num]['mean-var-estimate'][-1] for particle_num in
                             list(self.pf.particle_set.keys())]).flatten()
        pred_mean_var2 = np.sum(particle_mean_vars) / np.sum(np.array([self.pf.particle_set[particle_num]['log-weight'] for particle_num in
                             list(self.pf.particle_set.keys())]).flatten())
        print('2nd VAR ESTIMATE: = {}'.format(pred_mean_var2))




        fig101, ax101 = plt.subplots()
        ax101.plot(np.linspace(1, len(var_estimate_list), len(var_estimate_list)), np.array(var_estimate_list))
        fig101.suptitle('Var Estimate Evolution w/ timestep')
        plt.show()

        xx = np.linspace(0.1, 5, 10000)
        alpha = self.pf.rho + count / 2
        # gamma_alpha = gamma_func(alpha)  # part of the norm constant
        B = self.pf.eta + E[0][0] / 2
        # beta_to_alpha = (B) ** (alpha)  # part of the norm constant
        inv_gamma_pdf = []
        for x in xx:
            inv_gamma_pdf.append(-(alpha + 1) * np.log(x) - B / x)

        fig999, ax999 = plt.subplots()
        ax999.plot(xx, inv_gamma_pdf, 'r-', lw=1, alpha=0.6, label='invgamma pdf')
        fig999.suptitle('Inv Gamma Distr For Estimated Var')
        ax999.set_ylim(-300, None)
        plt.show()

        fig27, ax27 = plt.subplots()
        ax27.plot(np.linspace(1, len(ESS_list), len(ESS_list)), np.array(ESS_list))
        ax27.plot(np.linspace(1, len(ESS_list), len(ESS_list)), [np.log(pf.epsilon * num_particles)]*len(ESS_list), 'r--')
        fig27.suptitle('ESS Evolution')
        plt.show()

        self.jj_sig_post()


        self.display_pf_plots(state_mean, state_cov, var_estimate, times, observation_data, num_particles, beta, k_cov_init, known_real_states, show_particle_paths=True)


    def show_all_var_ests(self, varestlist):
        fig, ax = plt.subplots()
        for i in range(self.pf.Np):
            ax.plot(np.arange(0,len(varestlist[i])), np.array(varestlist[i]).flatten())
        plt.show()


    def jj_sig_post(self):
        lweights = np.array([self.pf.particle_set[particle_num]['log-weight'] for particle_num in
                             list(self.pf.particle_set.keys())]).flatten()
        Es = np.array([self.pf.particle_set[particle_num]['E'][-1]
                       for particle_num in list(self.pf.particle_set.keys())])
        count = int(self.pf.particle_set[1]['count'])
        mixture = np.zeros(10000)
        # mode = 0.
        # mean = 0.
        axis = np.linspace(0.1, 3, 10000)
        E = self.pf.log_sumexp_util(lweights, lambda x: x, Es, retlog=False)
        rhod = self.pf.rho + (count / 2.)
        etad = self.pf.eta + (E / 2.)
        sig_post = -(rhod + 1) * np.log(axis) - np.divide(etad, axis)
        mixture = sig_post
        mixture = mixture[0]
        # mode = (self.pf.eta + E / 2.) / (self.pf.rho + count / 2. + 1.)
        # mean = (self.pf.eta + E / 2.) / (self.pf.rho + count / 2. - 1.)
        fig90009, ax90009 = plt.subplots()
        ax90009.plot(axis, mixture)
        ax90009.plot(axis,
                     mixture - self.pf.log_sumexp_util(mixture, lambda x: 1., np.zeros(mixture.shape[0]), retlog=False))
        fig90009.suptitle('Inv Gamma - JJ METHOD')
        plt.show()

    def display_pf_plots(self, state_mean, state_cov, var_estimate, times, observation_data, num_particles, beta, k_cov_init, known_real_states, show_particle_paths):


        SCALE_FACTOR_ESTIMATE = 1
        print('manually scaling var by: {}'.format(SCALE_FACTOR_ESTIMATE))
        x0lower, x0upper, x0mid = self.compute_CIs(means=state_mean,
                                                   covs=SCALE_FACTOR_ESTIMATE * var_estimate * state_cov, stdevs=3,
                                                   state='x0')
        x1lower, x1upper, x1mid = self.compute_CIs(means=state_mean,
                                                   covs=SCALE_FACTOR_ESTIMATE * var_estimate * state_cov, stdevs=3,
                                                   state='x1')
        x2lower, x2upper, x2mid = self.compute_CIs(means=state_mean,
                                                   covs=SCALE_FACTOR_ESTIMATE * var_estimate * state_cov, stdevs=3,
                                                   state='x2')

        x0_pf_data = [x0lower, x0mid, x0upper]
        x1_pf_data = [x1lower, x1mid, x1upper]
        x2_pf_data = [x2lower, x2mid, x2upper]

        range_start = 0
        fig2, ax2 = plt.subplots()
        if known_real_states != False:
            ax2.scatter(times, self.pf.true_x_evo[0][:], color='r', s=4, zorder=2)
            ax2.plot(times, self.pf.true_x_evo[0][:], zorder=1, linestyle='--', color='r', alpha=0.7, label='True X0 State') # HUHJKHK

        ax2.plot(times[range_start:], x0mid[range_start:], linestyle='--', label='PF Mean')
        ax2.scatter(times[range_start:], observation_data[range_start:], marker='x', s=4, color='k',
                    label='Noisy State Observations')
        ax2.fill_between(times[range_start:], x0upper[range_start:], x0lower[range_start:], alpha=0.4,
                         label='PF +-99% CI')
        ax2.legend()
        fig2.suptitle(
            'X0 Evolution - Particle Filter - Updated - k_mu_BM = {}, kv = {}. Np = {} \n beta ={}, k_cov_init = {}'.format(self.k_mu_BM, self.kv,
                                                                                                              num_particles,
                                                                                                              beta,
                                                                                                              k_cov_init))
        plt.xlabel('Time')
        plt.ylabel('X0')
        plt.show()

        fig3, ax3 = plt.subplots()
        # ax1.scatter(obs_times, x_evolution[0][:], color='r', s=4, zorder=2)
        # ax2.plot(times, x_evolution[0][:], zorder=1, linestyle='--', color='r', alpha=0.7, label='True X0 State') # HUHJKHK

        ax3.plot(times[range_start:], x1mid[range_start:], linestyle='--', label='PF Mean')
        if known_real_states != False:
            ax3.scatter(times[:], self.pf.true_x_evo[1][:], marker='x', s=4, color='k', label='True X1 State Underlying')
            ax3.plot(times, self.pf.true_x_evo[1][:], linestyle='--', alpha=0.7, color='r', label='True X1 State Underlying')
        ax3.fill_between(times[range_start:], x1upper[range_start:], x1lower[range_start:], alpha=0.4,
                         label='PF +-99% CI')
        ax3.legend()
        fig3.suptitle(
            'X1 Evolution - Particle Filter - Updated - k_mu_BM = {}, kv = {}. Np = {}\n beta ={}, k_cov_init = {}'.format(self.k_mu_BM, self.kv,
                                                                                                             num_particles,
                                                                                                             beta,
                                                                                                             k_cov_init))
        plt.xlabel('Time')
        plt.ylabel('X1')
        plt.show()

        fig4, ax4 = plt.subplots()
        # ax1.scatter(obs_times, x_evolution[0][:], color='r', s=4, zorder=2)
        # ax2.plot(times, x_evolution[0][:], zorder=1, linestyle='--', color='r', alpha=0.7, label='True X0 State') # HUHJKHK

        ax4.plot(times[range_start:], x2mid[range_start:], linestyle='--', label='PF Mean')
        # ax3.scatter(times, pf.true_x_evo[1][:], marker='x', s=4, color='k', label='True X1 State Underlying')
        if known_real_states != False:
            ax4.plot(times, self.pf.true_x_evo[2][:], linestyle='--', alpha=0.7, color='r', label='True X2 State Underlying')
        ax4.fill_between(times[range_start:], x2upper[range_start:], x2lower[range_start:], alpha=0.4,
                         label='PF +-99% CI')
        ax4.legend()
        fig4.suptitle(
            'X2 - Particle Filter - k_mu_BM = {} kv = {}. Np = {} \n beta = {}, k_cov_init = {}'.format(self.k_mu_BM,
                                                                                                        self.kv,
                                                                                                        num_particles,
                                                                                                        beta,
                                                                                                        k_cov_init))
        plt.xlabel('Time')
        plt.ylabel('X2')
        plt.show()

        # self.plot_particles_2()

        # self.plot_2D_hist(x0_pf_data, x1_pf_data)
        # self.jj_plot()
        if show_particle_paths:
            self.show_results(x0_pf_data, x1_pf_data, x2_pf_data, times, observation_data, sigma_squared_mu=1)




    def show_results(self, x0data, x1data, x2data, times, observation_data, sigma_squared_mu=1):

        x0lower, x0mid, x0upper = x0data
        x1lower, x1mid, x1upper = x1data
        x2lower, x2mid, x2upper = x2data

        x0_unr = []
        x1_unr = []

        x0s = []
        x1s= []
        x2s = []
        threeDtimes = []
        for num in self.pf.particle_set.keys():
            particle_means = self.pf.state_evo_dict[num]

            state_traj = [state[0][0] for state in particle_means]
            state_traj_x1 = [state[1][0] for state in particle_means]
            state_traj_x2 = [state[2][0] for state in particle_means]

            x0s.append(state_traj)
            threeDtimes.append(list(times))

            x1s.append(state_traj_x1)
            x2s.append(state_traj_x2)

            for state in particle_means:
                x0 = state[0][0]
                x0_unr.append(x0)

                x1 = state[1][0]
                x1_unr.append(x1)

        # observation_data = self.pf.data_set_y_noisy_obs

        # times = self.pf.obs_times
        unr_times = list(times) * self.pf.Np

        fig, ax = plt.subplots()

        # Loop through each particle and create a line
        for i in range(len(x0s)):
            # Create a line for the current particle
            line, = ax.plot(threeDtimes[i][:len(x0s[i])], x0s[i], '-', linewidth=1, alpha=0.5, color='black')

        ax.plot(times, self.pf.true_x_evo[0][:], zorder=1, linestyle='--', color='r', alpha=0.7,
                  label='True X0 State')  # HUHJKHK
        #
        ax.plot(times, x0mid, linestyle='--', label='PF Mean')
        ax.scatter(times, observation_data, marker='x', s=4, color='k', label='Noisy State Observations')
        ax.fill_between(times, x0upper, x0lower, alpha=0.1, label='PF +-99% CI')
        ax.legend()
        fig.suptitle('X0 Evolution - Particle Filter - Updated - kv = {}. Np = {}'.format(self.kv, self.pf.Np))
        plt.xlabel('Time')
        plt.ylabel('X0')
        plt.show()

        fig, ax = plt.subplots()

        # Loop through each particle and create a line
        for i in range(len(x1s)):
            # Create a line for the current particle
            line, = ax.plot(threeDtimes[i][:len(x1s[i])], x1s[i], '-', linewidth=1, alpha=0.5, color='black')

        ax.plot(times, self.pf.true_x_evo[1][:], zorder=1, linestyle='--', color='r', alpha=0.7,
                label='True X1 State')  # HUHJKHK
        #
        ax.plot(times, x1mid, linestyle='--', label='PF Mean')
        # ax.scatter(times, observation_data, marker='x', s=4, color='k', label='Noisy State Observations')
        ax.fill_between(times, x1upper, x1lower, alpha=0.1, label='PF +-99% CI')
        ax.legend()
        fig.suptitle('X1 Evolution - Particle Filter - Updated - kv = {}. Np = {}'.format(self.kv, self.pf.Np))
        plt.xlabel('Time')
        plt.ylabel('X1')
        plt.show()

        fig, ax = plt.subplots()

        # Loop through each particle and create a line
        for i in range(len(x1s)):
            # Create a line for the current particle
            line, = ax.plot(threeDtimes[i][:len(x2s[i])], x2s[i], '-', linewidth=1, alpha=0.5, color='black')

        ax.plot(times, self.pf.true_x_evo[2][:], zorder=1, linestyle='--', color='r', alpha=0.7,
                label='True X2 State')  # HUHJKHK
        #
        ax.plot(times, x2mid, linestyle='--', label='PF Mean')
        # ax.scatter(times, observation_data, marker='x', s=4, color='k', label='Noisy State Observations')
        ax.fill_between(times, x2upper, x2lower, alpha=0.1, label='PF +-99% CI')
        ax.legend()
        fig.suptitle('X2 - Particle Filter - sigma_sq_mu_controlled = {} . kv = {}. Np = {}'.format(sigma_squared_mu, self.kv, self.pf.Np))
        plt.xlabel('Time')
        plt.ylabel('X2')
        plt.show()
